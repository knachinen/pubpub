---
title: A Survey on Retrieval-Augmented Text Generation for Large Language Models
draft: false
tags:
  - rag
  - llm
  - survey
---

## Information 

- [2404.10981] A Survey on Retrieval-Augmented Text Generation for Large Language Models (arxiv.org) | [Link](https://arxiv.org/abs/2404.10981) 
- Yizheng Huang, Jimmy Huang
- 2024-04 

## Abstract 

- LLM은 그럴듯하지만 잘못된 응답을 생성함
- RAG는 실제 데이터로 정확도와 신뢰도를 줌
- 4가지 RAG 패러다임 소개
	- 사전 검색 (pre-retrieval)
	- 검색 (retrieval)
	- 사후 검색 (post-retrieval)
	- 생성 (generation)
- 주요 연구를 분석하여 RAG 발전 과정을 소개
- RAG 평가 방법 소개 
- 앞에 놓여진 도전적인 과제와 앞으로의 연구 방향을 소개 
- 연구 목적
	- 정리된 프레임워크와 분류를 제공
	- 기존 연구들을 통합
	- 기술적 토대를 명확하게 함
	- 잠재성을 강조

## 1. Introduction 

- LLM의 신뢰도는 사후훈련으로 새로운 정보를 통합하는 것으로 제한됨
	- 여기에는 3가지 어려움이 있음
	- 첫째, 접근성과 적용성 때문에 일반적인 데이터에 중점을 두게 되어 전문 영역에서 수준 이하의 성능을 발휘함
	- 둘째, 데이터 주석을 달고 모델을 훈련시키려면 엄청난 자원을 써야되므로 모델을 지속적으로 업데이트하기 어려움
	- 셋째, 그럴듯하지만 부정확한 응답을 줌 = 환각(hallucination) 
- 하나의 해결 방법으로서, 검색증강생성(Retrieval Augmented Generation)
	- 쿼리검색으로 외부 데이터를 가져옴
	- 좀 더 정확하고 현재의 데이터임
	- 예를 들어 ChatGPT에 2032 올림픽이 어디에서 개최되는지 물으면, 2022년 1월이 마지막 업데이트라고 아직 공식적인 발표가 없다고 나옴. RAG 방법을 쓰면 위키피디아에서 정보를 찾아 응답할 수 있음.
- 2020년 Lewis 가 처음 소개
	- 그 뒤로 많은 발전이 있었음
	- 다양한 범위의 연구와 비슷한 방법에 애매한 용어를 씀
- 이 연구는 RAG의 명확한 overview 를 제공
	- 여러 방법들과 깊은 이해
	- 텍스트 RAG 에 집중
- RAG는 검색 방법 + 진보한 딥러닝
	- 관련된 정보를 효율적으로 검색
	- 정확한 응답을 생성
- 섹션 소개
	- 섹션 2에는 작업흐름과 4단계 분류
		- 사전 검색, 검색, 사후 검색, 생성
	- 섹션 3-6에서는 4단계의 심층 분석
	- 섹션 7에서는 검토한 연구들의 요약
	- 섹션 8에서는 평가 방법론을 설명
	- 섹션 9에서는 앞으로의 연구 방향
		- 텍스트, 이미지, 멀티모달
	- 섹션 10은 결론
- 이 연구는 세가지를 기여함
	- RAG의 종합적인 프레임워크, 개선점과 도전과제 인식
	- 핵심 기술의 자세한 분석. 검색과 생성의 강점 조사.
	- 평가 방법 소개. 현재의 도전과제와 앞으로의 연구방향 제안.
- 

## 2. RAG 프레임워크

- 사람의 감독 아래 실제 데이터를 사용함에 따라,
	- 생성 프로세스를 간단하게 함.
	- 생성 응답의 신뢰도를 향상시킴.
- Khandelwal 의 연구는 RAG 효율성을 강조하며 모델의 퍼포먼스를 개선시킬 수 있음을 증명.
- RAG는 보충 정보를 제공하는 수단에서 검색과 생성 사이에 다중 상호작용을 가능하게 하는 수단으로 발전.
	- 정확도를 높이기 위해 여러번의 검색과 생성
	- LangChain 과 LlamaIndex 같은 플랫폼이 이런 접근법을 사용하여 모듈화

### 2.1 기본 RAG 작업흐름 

- 작업흐름은 외부 소스들의 인덱싱을 생성하는 것으로 시작하여,
- 검색 모듈이 특정 쿼리를 기반으로 인덱싱된 관련 정보를 검색하고,
- 생성 모델이 검색된 정보로 결과를 생성함.

#### 2.1.1 인덱싱 

- 효율적인 검색은 데이터 준비가 핵심인 포괄적인 인덱싱으로 시작
- 인덱싱에 적합하게 토큰화, 형태소 분석, 불용어 제거 등 텍스트 정규화 과정을 거침
- 텍스트 조각을 문장이나 단락으로 구성 
	- 보다 집중적인 검색을 할 수 있고, 
	- 관련 키워드가 포함된 조각을 정확히 찾아낼 수 있음
- 사전훈련된 언어모델로 시맨틱 벡터 표현을 생성
	- 저장된 벡터로 빠르고 정확한 검색을 할 수 있음

#### 2.1.2 검색 

- 전통적인 검색 방법인 BM25 알고리즘 같은 것들은,
	- 문서 순위를 매길 때 용어 빈도와 존재 여부에만 초점을 둠
	- 쿼리의 의미 정보를 간과
- 현재 전략으로 BERT 같은 사전훈련된 언어모델은,
	- 쿼리의 의미를 더욱 효과적으로 잡아냄 
	- 동의어와 문장 구조를 고려하여 검색 정확도를 개선
	- 의미적 유사성으로 문서 순위를 개선 
	- 문서와 쿼리 사이의 벡터 거리를 잼
	- 전통적인 검색 평가 지표와 의미적 이해를 결합
	- 관련성이 있고 의도에 맞는 결과를 생성

#### 2.1.3 생성 

